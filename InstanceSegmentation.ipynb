{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "# Install PyTorch with CUDA 11.8\n",
    "!{sys.executable} -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install all other packages\n",
    "!{sys.executable} -m pip install \\\n",
    "  numpy matplotlib pillow \\\n",
    "  opencv-python \\\n",
    "  pycocotools \\\n",
    "  albumentations \\\n",
    "  scikit-image \\\n",
    "  pandas \\\n",
    "  ultralytics"
   ],
   "id": "dca59f78acae7641",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "import cv2\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn, MaskRCNN\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools import mask as maskUtils\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from ultralytics import YOLO"
   ],
   "id": "c474fbc9b95cdbb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ],
   "id": "5cff6c52645538e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e0c31ab5-a9c5-47ec-9c00-70c3dddae0eb",
   "metadata": {},
   "source": [
    "# 1. User parameters\n",
    "dataset_dir    = 'data'                  # ← change this\n",
    "INPUT_FILE     = 'annotations.json'  # ← your single annotations file\n",
    "TEST_PCT       = 0.10                    # 10% test\n",
    "VAL_PCT        = 0.10                    # 10% val\n",
    "RANDOM_SEED    = 42                      # for reproducibility\n",
    "\n",
    "# 2. Load the full annotation JSON\n",
    "with open(os.path.join(dataset_dir, INPUT_FILE), 'r') as f:\n",
    "    full = json.load(f)\n",
    "\n",
    "images            = full['images']\n",
    "annotations       = full.get('annotations', [])\n",
    "categories        = full.get('categories', [])\n",
    "\n",
    "# 3. Shuffle & split image list\n",
    "random.seed(RANDOM_SEED)\n",
    "random.shuffle(images)\n",
    "N = len(images)\n",
    "n_test = int(N * TEST_PCT + 0.5)\n",
    "n_val  = int(N * VAL_PCT  + 0.5)\n",
    "\n",
    "test_images  = images[:n_test]\n",
    "val_images   = images[n_test:n_test+n_val]\n",
    "train_images = images[n_test+n_val:]\n",
    "\n",
    "# 4. Build index sets for fast lookup\n",
    "test_ids  = {img['id'] for img in test_images}\n",
    "val_ids   = {img['id'] for img in val_images}\n",
    "train_ids = {img['id'] for img in train_images}\n",
    "\n",
    "# 5. Partition annotations\n",
    "def split_annotations(annotations, ids):\n",
    "    return [a for a in annotations if a['image_id'] in ids]\n",
    "\n",
    "train_anns = split_annotations(annotations, train_ids)\n",
    "val_anns = split_annotations(annotations, val_ids)\n",
    "test_anns = split_annotations(annotations, test_ids)\n",
    "\n",
    "# 6. Helper to dump one subset\n",
    "def create_subset(name, imgs, annotations):\n",
    "    out = {\n",
    "        'categories': categories,\n",
    "        'images': imgs,\n",
    "        'annotations': annotations,\n",
    "    }\n",
    "    path = os.path.join(dataset_dir, f'annotations_{name}.json')\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(out, f)\n",
    "    print(f'Wrote {name}: {len(imgs)} images, {len(annotations)} masks')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 7. Write three files\n",
    "create_subset('train', train_images, train_anns)\n",
    "create_subset('val', val_images, val_anns)\n",
    "create_subset('test', test_images, test_anns)"
   ],
   "id": "f570c167bc53b169",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2931090f-8ac5-4daf-a24a-4a467b642f92",
   "metadata": {},
   "source": [
    "ann_file    = os.path.join(dataset_dir, 'annotations_train.json')\n",
    "coco        = COCO(ann_file)\n",
    "img_ids     = coco.getImgIds()[:3]\n",
    "\n",
    "for img_id in img_ids:\n",
    "    # --- load image + anns ---\n",
    "    img = coco.loadImgs(img_id)[0]\n",
    "    I   = Image.open(os.path.join(dataset_dir, img['file_name']))\n",
    "    ann_ids = coco.getAnnIds(imgIds=[img_id])\n",
    "    anns    = coco.loadAnns(ann_ids)\n",
    "\n",
    "    # --- print how many + their categories ---\n",
    "    print(f\"\\nImage {img_id} ({img['file_name']}) has {len(anns)} masks:\")\n",
    "    for a in anns:\n",
    "        cat = coco.loadCats(a['category_id'])[0]\n",
    "        print(f\"  • {cat['name']} (super: {cat['supercategory']})\")\n",
    "\n",
    "    # --- set up figure ---\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,6))\n",
    "    fig.suptitle(f\"Image {img_id}\", fontsize=14)\n",
    "\n",
    "    ax1.imshow(I)\n",
    "    ax1.set_title(\"Original\")\n",
    "    ax1.axis('off')\n",
    "\n",
    "    ax2.imshow(I)\n",
    "    ax2.set_title(\"Colour‑coded masks\")\n",
    "    ax2.axis('off')\n",
    "\n",
    "    # --- overlay each mask in its own random colour ---\n",
    "    for ann in anns:\n",
    "        mask = coco.annToMask(ann)  # H×W binary\n",
    "        color = (random.random(), random.random(), random.random())\n",
    "        # make a H×W×3 RGB mask\n",
    "        rgb_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.float32)\n",
    "        for c in range(3):\n",
    "            rgb_mask[...,c] = mask * color[c]\n",
    "        ax2.imshow(rgb_mask, alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7c5cd3c0-d272-48df-9eb5-fdace219aa17",
   "metadata": {},
   "source": [
    "# === CONFIGURE THIS ===\n",
    "ann_file    = os.path.join(dataset_dir, 'annotations.json')\n",
    "\n",
    "# 1. Load your annotations\n",
    "with open(ann_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 2. Build a lookup for categories\n",
    "cats = {c['id']: c for c in data['categories']}\n",
    "\n",
    "# 3. Count up masks per category\n",
    "cat_counts = Counter()\n",
    "for ann in data['annotations']:\n",
    "    cat_counts[cats[ann['category_id']]['name']] += 1\n",
    "\n",
    "# sort descending\n",
    "cat_items = sorted(cat_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "names, counts = zip(*cat_items)\n",
    "\n",
    "# 3a. Print total masks and number of distinct categories\n",
    "total_masks      = sum(counts)\n",
    "num_categories   = len(names)\n",
    "print(f\"Total masks in dataset: {total_masks}\")\n",
    "print(f\"Number of categories   : {num_categories}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Plot horizontal bar chart\n",
    "plt.figure(figsize=(8, 12))\n",
    "plt.barh(names, counts)\n",
    "plt.gca().invert_yaxis()   # largest at the top\n",
    "plt.xlabel(\"Number of Masks\")\n",
    "plt.title(\"Masks per Category\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "548d5b6b5bb43445",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "89077d3c-a462-4dfc-83cf-c5829c2bd064",
   "metadata": {},
   "source": [
    "# === CONFIGURE THIS ===\n",
    "ann_file    = os.path.join(dataset_dir, 'annotations.json')\n",
    "\n",
    "# 1. Load your annotations\n",
    "with open(ann_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 2. Build a lookup for categories\n",
    "cats = {c['id']: c for c in data['categories']}\n",
    "\n",
    "# 3. Count up masks per supercategory\n",
    "super_counts = Counter()\n",
    "for ann in data['annotations']:\n",
    "    supercat = cats[ann['category_id']]['supercategory']\n",
    "    super_counts[supercat] += 1\n",
    "\n",
    "# 3a. Sort descending\n",
    "items = sorted(super_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "names, counts = zip(*items)\n",
    "\n",
    "# 3b. Print totals\n",
    "total_masks        = sum(counts)\n",
    "num_supercats      = len(names)\n",
    "print(f\"Total masks in dataset      : {total_masks}\")\n",
    "print(f\"Number of supercategories   : {num_supercats}\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Plot horizontal bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(names, counts)\n",
    "plt.gca().invert_yaxis()    # largest at the top\n",
    "plt.xlabel(\"Number of Masks\")\n",
    "plt.title(\"Masks per Supercategory\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "9b13538ff388da93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clip_boxes_to_image(boxes, img_w, img_h):\n",
    "    clipped = []\n",
    "    for box in boxes:\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "        x_min = max(0, min(x_min, img_w - 1))\n",
    "        y_min = max(0, min(y_min, img_h - 1))\n",
    "        x_max = max(0, min(x_max, img_w - 1))\n",
    "        y_max = max(0, min(y_max, img_h - 1))\n",
    "        clipped.append([x_min, y_min, x_max, y_max])\n",
    "    return clipped"
   ],
   "id": "f8256d453e708e78",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40bbe99f-1615-4da8-b4a5-39ec00f17fc8",
   "metadata": {},
   "source": [
    "class TacoDataset(Dataset):\n",
    "    def __init__(self, images_dir, annotation_path, transforms=None, max_resolution=(4000, 4000)):\n",
    "        self.images_dir = images_dir\n",
    "        self.coco = COCO(annotation_path)\n",
    "        self.transforms = transforms\n",
    "        self.max_resolution = max_resolution\n",
    "        self.image_ids = [img_id for img_id in self.coco.imgs\n",
    "                          if self.coco.imgs[img_id]['width'] <= max_resolution[0] and\n",
    "                             self.coco.imgs[img_id]['height'] <= max_resolution[1]]\n",
    "        print(f\"Loaded {len(self.image_ids)} images under resolution {max_resolution}\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.image_ids[index]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.images_dir, img_info['file_name'])\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        img_h, img_w = image.shape[:2]\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        boxes, labels, masks = [], [], []\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            x_min, y_min = x, y\n",
    "            x_max, y_max = x + w, y + h\n",
    "            if x_max <= x_min or y_max <= y_min:\n",
    "                continue\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            labels.append(ann['category_id'])\n",
    "            masks.append(self.coco.annToMask(ann))\n",
    "\n",
    "        boxes = clip_boxes_to_image(boxes, img_w, img_h)\n",
    "\n",
    "        resized_masks = []\n",
    "        for m in masks:\n",
    "            if m.shape != (img_h, img_w):\n",
    "                m_img = Image.fromarray(m.astype(np.uint8))\n",
    "                m_resized = F.resize(m_img, (img_h, img_w), interpolation=Image.NEAREST)\n",
    "                m = np.array(m_resized)\n",
    "            resized_masks.append(m)\n",
    "\n",
    "        if not boxes:\n",
    "            boxes = [[0, 0, 1, 1]]\n",
    "            labels = [0]\n",
    "            resized_masks = [np.zeros((img_h, img_w), dtype=np.uint8)]\n",
    "\n",
    "        if self.transforms:\n",
    "            try:\n",
    "                transformed = self.transforms(\n",
    "                    image=image,\n",
    "                    masks=resized_masks,\n",
    "                    bboxes=boxes,\n",
    "                    category_ids=labels\n",
    "                )\n",
    "\n",
    "                valid_boxes, valid_labels, valid_masks = [], [], []\n",
    "                for i, box in enumerate(transformed[\"bboxes\"]):\n",
    "                    x_min, y_min, x_max, y_max = box\n",
    "                    if (x_max - x_min) > 1 and (y_max - y_min) > 1:\n",
    "                        valid_boxes.append(box)\n",
    "                        valid_labels.append(transformed[\"category_ids\"][i])\n",
    "                        valid_masks.append(transformed[\"masks\"][i])\n",
    "\n",
    "                if not valid_boxes:\n",
    "                    h, w = transformed[\"image\"].shape[1:]\n",
    "                    valid_boxes = [[0, 0, 1, 1]]\n",
    "                    valid_labels = [0]\n",
    "                    valid_masks = [np.zeros((h, w), dtype=np.uint8)]\n",
    "\n",
    "                image = transformed['image'].float() / 255.0\n",
    "                boxes = torch.as_tensor(valid_boxes, dtype=torch.float32)\n",
    "                labels = torch.as_tensor(valid_labels, dtype=torch.int64)\n",
    "                masks = torch.stack([torch.tensor(m, dtype=torch.uint8) for m in valid_masks])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Transform failed on image_id {img_id} with error: {e}\")\n",
    "                image = F.to_tensor(Image.fromarray(image))\n",
    "            \n",
    "                # Filter invalid boxes again (even after failed transform)\n",
    "                final_boxes, final_labels, final_masks = [], [], []\n",
    "                for i, box in enumerate(boxes):\n",
    "                    x_min, y_min, x_max, y_max = box\n",
    "                    if (x_max - x_min) > 1 and (y_max - y_min) > 1:\n",
    "                        final_boxes.append(box)\n",
    "                        final_labels.append(labels[i])\n",
    "                        final_masks.append(resized_masks[i])\n",
    "            \n",
    "                if not final_boxes:\n",
    "                    final_boxes = [[0, 0, 1, 1]]\n",
    "                    final_labels = [0]\n",
    "                    final_masks = [np.zeros((img_h, img_w), dtype=np.uint8)]\n",
    "\n",
    "                boxes = torch.as_tensor(final_boxes, dtype=torch.float32)\n",
    "                labels = torch.as_tensor(final_labels, dtype=torch.int64)\n",
    "                masks = torch.stack([torch.tensor(m, dtype=torch.uint8) for m in final_masks])\n",
    "\n",
    "        else:\n",
    "            image = F.to_tensor(Image.fromarray(image))\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            masks = torch.as_tensor(np.stack(resized_masks), dtype=torch.uint8)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'masks': masks,\n",
    "            'image_id': torch.tensor([img_id]),\n",
    "            'area': torch.as_tensor([ann['area'] for ann in anns], dtype=torch.float32),\n",
    "            'iscrowd': torch.as_tensor([ann.get('iscrowd', 0) for ann in anns], dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e05a6bb-4384-48d2-b576-3bc705ba06bc",
   "metadata": {},
   "source": [
    "# ✅ New augmentations using Detectron2-style (simple and robust)\n",
    "# def get_train_transform():\n",
    "#     return A.Compose([\n",
    "#         A.Resize(512, 512),\n",
    "#         A.HorizontalFlip(p=0.5),\n",
    "#         A.RandomBrightnessContrast(p=0.2),\n",
    "#         ToTensorV2()\n",
    "#     ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category_ids']))\n",
    "\n",
    "# taruns changes\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(512, 512),  # or use A.LongestMaxSize(512) + A.PadIfNeeded if keeping aspect ratio\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.2),\n",
    "        A.RandomRotate90(p=0.3),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, border_mode=0, p=0.5),\n",
    "\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0)),\n",
    "            A.ISONoise(p=1),\n",
    "            A.MultiplicativeNoise(p=1),\n",
    "        ], p=0.3),\n",
    "\n",
    "        A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1, p=0.3),\n",
    "        A.RandomBrightnessContrast(p=0.3),\n",
    "\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # optional: pretrained normalization\n",
    "        ToTensorV2()\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category_ids'])\n",
    "    )\n",
    "\n",
    "\n",
    "def get_val_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(512, 512),\n",
    "        ToTensorV2()\n",
    "    ])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Dataset & DataLoader setup\n",
    "train_dataset = TacoDataset(\n",
    "    images_dir='data',\n",
    "    annotation_path='data/annotations_train.json',\n",
    "    transforms=get_train_transform()\n",
    ")\n",
    "\n",
    "val_dataset = TacoDataset(\n",
    "    images_dir='data',\n",
    "    annotation_path='data/annotations_val.json',\n",
    "    transforms=get_val_transform()\n",
    ")\n",
    "\n",
    "test_dataset = TacoDataset(\n",
    "    images_dir='data',\n",
    "    annotation_path='data/annotations_test.json',\n",
    "    transforms=get_val_transform()\n",
    ")"
   ],
   "id": "9350096a663ccbb0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"\\nTrain: {len(train_dataset)} images\")\n",
    "print(f\"Val:   {len(val_dataset)} images\")\n",
    "print(f\"Test:  {len(test_dataset)} images\")"
   ],
   "id": "29c670a5941dd7eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def show_image_with_boxes_masks(image, boxes, masks, title=\"\"):\n",
    "    img = image.copy()\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "\n",
    "    for mask in masks:\n",
    "        img[mask > 0] = img[mask > 0] * 0.5 + np.array([255, 0, 0]) * 0.5  # overlay red mask\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(img.astype(np.uint8))\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ],
   "id": "4285dd8ef8b4b010",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "985454d4-4a1d-4fe1-9edc-c25b9d2d24e6",
   "metadata": {},
   "source": [
    "# Pick a random image\n",
    "index = random.randint(0, len(train_dataset) - 1)\n",
    "\n",
    "# Load without transform\n",
    "original = TacoDataset(\n",
    "    images_dir='data',\n",
    "    annotation_path='data/annotations_train.json',\n",
    "    transforms=None\n",
    ")[index]\n",
    "\n",
    "orig_img = original[0].permute(1, 2, 0).numpy() * 255\n",
    "orig_boxes = original[1]['boxes'].numpy()\n",
    "orig_masks = original[1]['masks'].numpy()\n",
    "\n",
    "# Load with transform\n",
    "augmented = train_dataset[index]\n",
    "aug_img = augmented[0].permute(1, 2, 0).numpy() * 255\n",
    "aug_boxes = augmented[1]['boxes'].numpy()\n",
    "aug_masks = augmented[1]['masks'].numpy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Show before\n",
    "show_image_with_boxes_masks(orig_img, orig_boxes, orig_masks, title=\"Before Augmentation\")\n",
    "\n",
    "# Show after\n",
    "show_image_with_boxes_masks(aug_img, aug_boxes, aug_masks, title=\"After Augmentation\")"
   ],
   "id": "fd4bde0203c349fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "809380e1-b085-41df-bfa8-83d89048032c",
   "metadata": {},
   "source": [
    "images, targets = next(iter(train_loader))\n",
    "\n",
    "print(f'Batch size: {len(images)}')\n",
    "print(f'Image shape: {images[0].shape}')\n",
    "print(f'Target keys: {targets[0].keys()}')\n",
    "\n",
    "# Now loop through each key-value pair in the first target\n",
    "print(\"\\n--- Target values ---\")\n",
    "for key, value in targets[0].items():\n",
    "    print(f\"{key}: {value}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6ad4902d-2dd6-42d8-a0e3-a3a8280a6869",
   "metadata": {},
   "source": [
    "# --- Create the Mask R-CNN model ---\n",
    "def get_model(num_classes):\n",
    "    backbone = resnet_fpn_backbone('resnet50', pretrained=True)\n",
    "    model = MaskRCNN(backbone, num_classes=num_classes)\n",
    "\n",
    "    # Replace box head\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Replace mask head\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, 256, num_classes)\n",
    "    \n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Setup ---\n",
    "num_classes = len(train_dataset.coco.getCatIds())# + 1  # +1 for background\n",
    "print(f'Classes: {num_classes}')"
   ],
   "id": "3f205c8247b7c7a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5a06da69-2886-46fe-9044-2412c9b88dab",
   "metadata": {},
   "source": [
    "# base model\n",
    "model = get_model(num_classes).to(device)\n",
    "\n",
    "print(f'Model is on device: {device}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_model(num_classes, model_name=\"resnet50\", yolo_variant=\"yolov8n-seg.pt\"):\n",
    "    \"\"\"\n",
    "    Return a detection model. Supports torchvision Mask R-CNN and YOLOv8.\n",
    "\n",
    "    Parameters:\n",
    "    - num_classes: int\n",
    "    - model_name: str — options: resnet50, resnet101, mobilenet, custom_resnet50, yolo\n",
    "    - yolo_variant: str — YOLOv8 model file (used only if model_name=\"yolo\")\n",
    "\n",
    "    Returns:\n",
    "    - model (torch.nn.Module or Ultralytics YOLO object)\n",
    "    \"\"\"\n",
    "    if model_name == \"resnet50\":\n",
    "        backbone = resnet_fpn_backbone('resnet50', pretrained=True)\n",
    "        model = MaskRCNN(backbone, num_classes=num_classes)\n",
    "\n",
    "    elif model_name == \"resnet101\":\n",
    "        backbone = resnet_fpn_backbone('resnet101', pretrained=True)\n",
    "        model = MaskRCNN(backbone, num_classes=num_classes)\n",
    "\n",
    "    elif model_name == \"mobilenet\":\n",
    "        backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "        backbone.out_channels = 1280\n",
    "        model = MaskRCNN(backbone, num_classes=num_classes)\n",
    "\n",
    "    elif model_name == \"custom_resnet50\":\n",
    "        backbone = resnet_fpn_backbone(\"resnet50\", pretrained=True, trainable_layers=3)\n",
    "        model = MaskRCNN(backbone, num_classes=num_classes)\n",
    "\n",
    "    elif model_name == \"yolo\":\n",
    "        if YOLO is None:\n",
    "            raise ImportError(\"Ultralytics is not installed. Run `pip install ultralytics` first.\")\n",
    "        model = YOLO(yolo_variant)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "    # Only modify heads for torchvision models\n",
    "    if model_name != \"yolo\":\n",
    "        # Replace box head\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "        # Replace mask head\n",
    "        in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "        model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, 256, num_classes)\n",
    "\n",
    "    return model"
   ],
   "id": "9a0d5cad878cbf5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Torchvision Mask R-CNN\n",
    "model = get_model(num_classes=num_classes, model_name=\"resnet101\").to(device)\n",
    "model\n",
    "\n",
    "# YOLOv8 with Ultralytics\n",
    "# model = get_model(num_classes=5, model_name=\"yolo\", yolo_variant=\"yolov8s-seg.pt\")"
   ],
   "id": "171a4d0bd01caae3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "optimizer = optim.SGD(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    lr=0.005, # 0.001, 0.005, 0.0001\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005)\n",
    "\n",
    "# lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=16)"
   ],
   "id": "9c1d3657a9f496a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "95653340-81cf-4db8-96f1-071989767f82",
   "metadata": {},
   "source": [
    "# from skimage import measure\n",
    "\n",
    "# def binary_mask_to_polygon(mask, tolerance=2):\n",
    "#     if mask.shape[0] < 2 or mask.shape[1] < 2:\n",
    "#         print(f\"⚠️ Skipping too-small mask with shape: {mask.shape}\")\n",
    "#         return []  # Skip invalid masks\n",
    "\n",
    "#     contours = measure.find_contours(mask, 0.5)\n",
    "#     segmentations = []\n",
    "\n",
    "#     for contour in contours:\n",
    "#         contour = np.flip(contour, axis=1)  # (y,x) → (x,y)\n",
    "#         segmentation = contour.ravel().tolist()\n",
    "#         if len(segmentation) >= 6:  # must have at least 3 points\n",
    "#             segmentations.append(segmentation)\n",
    "\n",
    "#     return segmentations"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "defd3e40-f779-490f-b8db-f960d4aa59d5",
   "metadata": {},
   "source": [
    "# from pycocotools.cocoeval import COCOeval\n",
    "# import numpy as np\n",
    "\n",
    "# def evaluate_model(model, data_loader, coco_gt, device, coco_category_ids):\n",
    "#     model.eval()\n",
    "#     coco_results = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, targets in data_loader:\n",
    "#             images = [img.to(device) for img in images]\n",
    "#             outputs = model(images)\n",
    "\n",
    "#             for i, output in enumerate(outputs):\n",
    "#                 img_id = int(targets[i][\"image_id\"].item())\n",
    "#                 boxes = output[\"boxes\"].detach().cpu().numpy()\n",
    "#                 scores = output[\"scores\"].detach().cpu().numpy()\n",
    "#                 labels = output[\"labels\"].detach().cpu().numpy()\n",
    "#                 masks = output[\"masks\"].detach().cpu().numpy()[:, 0, :, :] \n",
    "\n",
    "#                 for j in range(len(boxes)):\n",
    "#                     # if scores[j] < 0.05:\n",
    "#                     #     continue\n",
    "\n",
    "#                     x1, y1, x2, y2 = boxes[j]\n",
    "#                     if x2 <= x1 or y2 <= y1:\n",
    "#                         continue\n",
    "\n",
    "#                     polygons = binary_mask_to_polygon(masks[j])\n",
    "#                     if not polygons:\n",
    "#                         continue\n",
    "\n",
    "#                     coco_results.append({\n",
    "#                         \"image_id\": img_id,\n",
    "#                         \"category_id\": int(coco_category_ids.get(labels[j], labels[j])),  # map label\n",
    "#                         \"bbox\": [x1, y1, x2 - x1, y2 - y1],\n",
    "#                         \"score\": 1.0,\n",
    "#                         \"segmentation\": polygons\n",
    "#                     })\n",
    "\n",
    "#     if not coco_results:\n",
    "#         print(\"⚠️ No valid predictions to evaluate.\")\n",
    "#         return\n",
    "\n",
    "#     coco_dt = coco_gt.loadRes(coco_results)\n",
    "#     coco_eval = COCOeval(coco_gt, coco_dt, iouType='segm')\n",
    "#     coco_eval.evaluate()\n",
    "#     coco_eval.accumulate()\n",
    "#     coco_eval.summarize()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "58e3eacc-7b65-4267-b8b0-6e2144d24d78",
   "metadata": {},
   "source": [
    "def calculate_iou(mask1, mask2):\n",
    "    intersection = np.logical_and(mask1, mask2).sum()\n",
    "    union = np.logical_or(mask1, mask2).sum()\n",
    "    return intersection / union if union > 0 else 0.0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "db7b7ef9-2848-49a7-89d7-ea1473be096e",
   "metadata": {},
   "source": [
    "# def evaluate_segmentation_metrics(model, data_loader, device, iou_threshold=0.5):\n",
    "#     model.eval()\n",
    "#     all_ious = []\n",
    "#     true_positives, false_positives, false_negatives = 0, 0, 0\n",
    "#     cls_correct, cls_total = 0, 0\n",
    "#\n",
    "#     with torch.no_grad():\n",
    "#         for images, targets in data_loader:\n",
    "#             images = [img.to(device) for img in images]\n",
    "#\n",
    "#             outputs = model(images)\n",
    "#             outputs = [{k: v.cpu() for k, v in out.items()} for out in outputs]\n",
    "#\n",
    "#             torch.cuda.empty_cache()\n",
    "#\n",
    "#             for i in range(len(images)):\n",
    "#                 gt_masks = targets[i]['masks'].cpu().numpy().astype(bool)\n",
    "#                 gt_labels = targets[i]['labels'].cpu().numpy()\n",
    "#                 pred_masks = outputs[i]['masks'].numpy() > 0.5\n",
    "#                 pred_masks = pred_masks[:, 0, :, :]\n",
    "#                 pred_labels = outputs[i]['labels'].numpy()\n",
    "#\n",
    "#                 matched_gt = set()\n",
    "#                 matched_pred = set()\n",
    "#\n",
    "#                 for gi, (gt, gt_cls) in enumerate(zip(gt_masks, gt_labels)):\n",
    "#                     best_iou = 0\n",
    "#                     best_pi = -1\n",
    "#                     for pi, (pred, pred_cls) in enumerate(zip(pred_masks, pred_labels)):\n",
    "#                         if pred_cls != gt_cls:\n",
    "#                             continue  # only match masks of same category\n",
    "#                         iou = calculate_iou(gt, pred)\n",
    "#                         if iou > best_iou:\n",
    "#                             best_iou = iou\n",
    "#                             best_pi = pi\n",
    "#\n",
    "#                     if best_iou >= iou_threshold:\n",
    "#                         matched_gt.add(gi)\n",
    "#                         matched_pred.add(best_pi)\n",
    "#                         true_positives += 1\n",
    "#                         all_ious.append(best_iou)\n",
    "#                         # Classification accuracy\n",
    "#                         if pred_labels[best_pi] == gt_cls:\n",
    "#                             cls_correct += 1\n",
    "#                         cls_total += 1\n",
    "#                     else:\n",
    "#                         false_negatives += 1\n",
    "#                         cls_total += 1\n",
    "#\n",
    "#                 for pi in range(len(pred_masks)):\n",
    "#                     if pi not in matched_pred:\n",
    "#                         false_positives += 1\n",
    "#                         cls_total += 1\n",
    "#\n",
    "#             del images, targets, outputs\n",
    "#             torch.cuda.empty_cache()\n",
    "#\n",
    "#     precision = true_positives / (true_positives + false_positives + 1e-6)\n",
    "#     recall = true_positives / (true_positives + false_negatives + 1e-6)\n",
    "#     mean_iou = np.mean(all_ious) if all_ious else 0.0\n",
    "#     cls_acc = cls_correct / (cls_total + 1e-6)\n",
    "#\n",
    "#     return {\n",
    "#         \"mean_iou\": mean_iou,\n",
    "#         \"precision\": precision,\n",
    "#         \"recall\": recall,\n",
    "#         \"cls_accuracy\": cls_acc,\n",
    "#         \"num_predictions\": true_positives + false_positives,\n",
    "#         \"num_ground_truth\": true_positives + false_negatives,\n",
    "#     }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "74d0aa06-2862-48cc-bfbd-5c3b3bc780ca",
   "metadata": {},
   "source": [
    "# def train_and_evaluate(model, train_loader, val_loader, test_loader, optimizer, lr_scheduler, device, num_epochs=10, print_interval=20):\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         total_loss = 0.0\n",
    "#         print(f\"\\n\\U0001F680 Starting Epoch {epoch+1}/{num_epochs} at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "#\n",
    "#         for step, (images, targets) in enumerate(train_loader):\n",
    "#             images = [img.to(device) for img in images]\n",
    "#             targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "#\n",
    "#             try:\n",
    "#                 loss_dict = model(images, targets)\n",
    "#                 loss = sum(loss for loss in loss_dict.values())\n",
    "#\n",
    "#                 if torch.isnan(loss) or torch.isinf(loss):\n",
    "#                     print(f\"\\u26A0\\ufe0f Invalid loss at step {step+1}, skipping\")\n",
    "#                     continue\n",
    "#\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 total_loss += loss.item()\n",
    "#             except RuntimeError as e:\n",
    "#                 print(f\"\\u274C CUDA error at step {step+1}: {e}\")\n",
    "#                 torch.cuda.empty_cache()\n",
    "#                 continue\n",
    "#\n",
    "#             if (step + 1) % print_interval == 0 or (step + 1) == len(train_loader):\n",
    "#                 print(f\"  \\U0001F501 Step {step+1}/{len(train_loader)} — Batch Loss: {loss.item():.4f}\")\n",
    "#\n",
    "#             del images, targets\n",
    "#             torch.cuda.empty_cache()\n",
    "#\n",
    "#         lr_scheduler.step()\n",
    "#         avg_loss = total_loss / len(train_loader)\n",
    "#         print(f\"\\n\\U0001F4D8 Epoch {epoch+1} — Average Epoch Loss: {avg_loss:.4f}\")\n",
    "#         if epoch%2==0:\n",
    "#             for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "#                 print(f\"\\n\\U0001F50D Evaluating on {name} set...\")\n",
    "#                 try:\n",
    "#                     metrics = evaluate_segmentation_metrics(model, loader, device)\n",
    "#                     print(f\"\\U0001F4CF {name.upper()} → IoU: {metrics['mean_iou']:.4f} | \"\n",
    "#                           f\"Precision: {metrics['precision']:.4f} | Recall: {metrics['recall']:.4f} | \"\n",
    "#                           f\"Class Acc: {metrics['cls_accuracy']:.4f}\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"\\u274C Evaluation failed on {name}: {e}\")\n",
    "#\n",
    "#     print(\"\\n\\U0001F50D Final Evaluation on TEST set...\")\n",
    "#     metrics_test = evaluate_segmentation_metrics(model, test_loader, device)\n",
    "#     print(f\"\\U0001F4CF TEST → IoU: {metrics_test['mean_iou']:.4f} | \"\n",
    "#           f\"Precision: {metrics_test['precision']:.4f} | Recall: {metrics_test['recall']:.4f} | \"\n",
    "#           f\"Class Acc: {metrics_test['cls_accuracy']:.4f}\")\n",
    "#\n",
    "#     torch.save(model.state_dict(), \"maskrcnn_taco_baseline.pth\")\n",
    "#     print(\"\\u2705 Model saved to maskrcnn_taco_baseline.pth\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# taruns changes\n",
    "\n",
    "def evaluate_segmentation_metrics(model, data_loader, device, iou_threshold=0.5):\n",
    "    model.eval()\n",
    "    all_ious = []\n",
    "    true_positives, false_positives, false_negatives = 0, 0, 0\n",
    "    cls_correct, cls_total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(data_loader, desc=\"Evaluating\", leave=False)\n",
    "\n",
    "        for images, targets in progress_bar:\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "            outputs = [{k: v.cpu() for k, v in out.items()} for out in outputs]\n",
    "\n",
    "            for i in range(len(images)):\n",
    "                gt_masks = targets[i]['masks'].cpu().numpy().astype(bool)\n",
    "                gt_labels = targets[i]['labels'].cpu().numpy()\n",
    "                pred_masks = outputs[i]['masks'].numpy() > 0.5\n",
    "                pred_masks = pred_masks[:, 0, :, :]\n",
    "                pred_labels = outputs[i]['labels'].numpy()\n",
    "\n",
    "                matched_gt = set()\n",
    "                matched_pred = set()\n",
    "\n",
    "                for gi, (gt, gt_cls) in enumerate(zip(gt_masks, gt_labels)):\n",
    "                    best_iou = 0\n",
    "                    best_pi = -1\n",
    "                    for pi, (pred, pred_cls) in enumerate(zip(pred_masks, pred_labels)):\n",
    "                        if pred_cls != gt_cls:\n",
    "                            continue\n",
    "                        iou = calculate_iou(gt, pred)\n",
    "                        if iou > best_iou:\n",
    "                            best_iou = iou\n",
    "                            best_pi = pi\n",
    "\n",
    "                    if best_iou >= iou_threshold:\n",
    "                        matched_gt.add(gi)\n",
    "                        matched_pred.add(best_pi)\n",
    "                        true_positives += 1\n",
    "                        all_ious.append(best_iou)\n",
    "                        if pred_labels[best_pi] == gt_cls:\n",
    "                            cls_correct += 1\n",
    "                        cls_total += 1\n",
    "                    else:\n",
    "                        false_negatives += 1\n",
    "                        cls_total += 1\n",
    "\n",
    "                for pi in range(len(pred_masks)):\n",
    "                    if pi not in matched_pred:\n",
    "                        false_positives += 1\n",
    "                        cls_total += 1\n",
    "\n",
    "            # 🧹 Clean up per batch\n",
    "            del images, targets, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives + 1e-6)\n",
    "    recall = true_positives / (true_positives + false_negatives + 1e-6)\n",
    "    mean_iou = np.mean(all_ious) if all_ious else 0.0\n",
    "    cls_acc = cls_correct / (cls_total + 1e-6)\n",
    "\n",
    "    # 🧹 Final cleanup after full evaluation\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return {\n",
    "        \"mean_iou\": mean_iou,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"cls_accuracy\": cls_acc,\n",
    "    }"
   ],
   "id": "dbae37ade2c987a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# taruns changes\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, test_loader, optimizer, lr_scheduler, device, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} started - {datetime.now().strftime('%H:%M:%S')}\")\n",
    "\n",
    "        # Use tqdm for progress bar\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Training]\", leave=False)\n",
    "\n",
    "        for step, (images, targets) in enumerate(progress_bar):\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            try:\n",
    "                loss_dict = model(images, targets)\n",
    "                loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    progress_bar.write(f\"Warning: Invalid loss at batch {step+1}, skipping batch.\")\n",
    "                    continue\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Update progress bar with latest batch loss\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                progress_bar.write(f\"RuntimeError at batch {step+1}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # 🧹 Free up memory for each batch\n",
    "            del images, targets, loss_dict, loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} finished. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # 🧹 Free cache after epoch\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Evaluate after every epoch\n",
    "        for name, loader in [(\"Train\", train_loader), (\"Val\", val_loader)]:\n",
    "            print(f\"\\nEvaluating {name} set...\")\n",
    "            try:\n",
    "                metrics = evaluate_segmentation_metrics(model, loader, device)\n",
    "                print(f\"{name} Metrics — IoU: {metrics['mean_iou']:.4f}, \"\n",
    "                      f\"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, \"\n",
    "                      f\"Cls Acc: {metrics['cls_accuracy']:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Evaluation error on {name}: {e}\")\n",
    "\n",
    "            # 🧹 Free cache after each evaluation\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Final Test Evaluation\n",
    "    print(\"\\nFinal Evaluation on TEST set:\")\n",
    "    metrics_test = evaluate_segmentation_metrics(model, test_loader, device)\n",
    "    print(f\"TEST Metrics — IoU: {metrics_test['mean_iou']:.4f}, \"\n",
    "          f\"Precision: {metrics_test['precision']:.4f}, Recall: {metrics_test['recall']:.4f}, \"\n",
    "          f\"Cls Acc: {metrics_test['cls_accuracy']:.4f}\")\n",
    "\n",
    "    # 🧹 Free cache after final test evaluation\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), \"maskrcnn_taco_baseline.pth\")\n",
    "    print(\"Model saved to maskrcnn_taco_baseline.pth\")"
   ],
   "id": "be38e44b0e7f3f50",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "97cdf592-b1ef-4d6e-af09-8a1943da1ec7",
   "metadata": {},
   "source": "train_and_evaluate(model, train_loader, val_loader, test_loader, optimizer, lr_scheduler, device, num_epochs=10)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9be8e05e-a14f-4a15-a515-53b13bc7858d",
   "metadata": {},
   "source": [
    "# def evaluate_ground_truth_as_prediction(coco_gt):\n",
    "#     coco_results = []\n",
    "#\n",
    "#     for img_id in coco_gt.getImgIds():\n",
    "#         anns = coco_gt.loadAnns(coco_gt.getAnnIds(imgIds=img_id))\n",
    "#         for ann in anns:\n",
    "#             if 'segmentation' not in ann or 'bbox' not in ann or 'category_id' not in ann:\n",
    "#                 continue\n",
    "#\n",
    "#             # COCO format expects score when evaluating detections\n",
    "#             coco_results.append({\n",
    "#                 \"image_id\": img_id,\n",
    "#                 \"category_id\": ann[\"category_id\"],\n",
    "#                 \"bbox\": ann[\"bbox\"],\n",
    "#                 \"score\": 1.0,  # max confidence\n",
    "#                 \"segmentation\": ann[\"segmentation\"]\n",
    "#             })\n",
    "#\n",
    "#     # Convert to COCO predictions format\n",
    "#     coco_dt = coco_gt.loadRes(coco_results)\n",
    "#\n",
    "#     # Run evaluation\n",
    "#     coco_eval = COCOeval(coco_gt, coco_dt, iouType='segm')\n",
    "#     coco_eval.evaluate()\n",
    "#     coco_eval.accumulate()\n",
    "#     coco_eval.summarize()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d70eb00a-a80e-4185-94da-d647d4b196c8",
   "metadata": {},
   "source": "# evaluate_ground_truth_as_prediction(val_dataset.coco)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_ground_truth_as_prediction(coco_gt, iou_type='segm'):\n",
    "    \"\"\"\n",
    "    Evaluate ground truth annotations against themselves using COCO evaluation.\n",
    "\n",
    "    Args:\n",
    "        coco_gt (COCO): Ground truth COCO object\n",
    "        iou_type (str): Type of evaluation - 'bbox' or 'segm'\n",
    "\n",
    "    Returns:\n",
    "        COCOeval object with results\n",
    "    \"\"\"\n",
    "    if len(coco_gt.getImgIds()) == 0:\n",
    "        print(\"No images found in COCO dataset.\")\n",
    "        return None\n",
    "\n",
    "    coco_results = []\n",
    "    for img_id in coco_gt.getImgIds():\n",
    "        anns = coco_gt.loadAnns(coco_gt.getAnnIds(imgIds=img_id))\n",
    "        for ann in anns:\n",
    "            if not all(k in ann for k in ('segmentation', 'bbox', 'category_id')):\n",
    "                continue\n",
    "\n",
    "            coco_results.append({\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": ann[\"category_id\"],\n",
    "                \"bbox\": ann[\"bbox\"],\n",
    "                \"score\": 1.0,  # perfect confidence\n",
    "                \"segmentation\": ann[\"segmentation\"]\n",
    "            })\n",
    "\n",
    "    if not coco_results:\n",
    "        print(\"No valid annotations found to evaluate.\")\n",
    "        return None\n",
    "\n",
    "    # Load results in COCO format\n",
    "    coco_dt = coco_gt.loadRes(coco_results)\n",
    "\n",
    "    # Perform evaluation\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType=iou_type)\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    return coco_eval"
   ],
   "id": "a9405aa0efb79f17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# coco_gt = COCO('data/annotations_val.json')\n",
    "coco_gt = val_dataset.coco\n",
    "\n",
    "print(f'Evaluate masks (segmentation)')\n",
    "evaluate_ground_truth_as_prediction(coco_gt, iou_type='segm')\n",
    "\n",
    "print(f'Evaluate bounding boxes')\n",
    "evaluate_ground_truth_as_prediction(coco_gt, iou_type='bbox')"
   ],
   "id": "732ab9365a633415",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
