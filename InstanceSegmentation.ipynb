{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c31ab5-a9c5-47ec-9c00-70c3dddae0eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "# 1. User parameters\n",
    "DATASET_DIR    = 'data'    # ← change this\n",
    "INPUT_FILE     = 'annotations.json'      # ← your single annotations file\n",
    "TEST_PCT       = 0.10                    # 10% test\n",
    "VAL_PCT        = 0.10                    # 10% val\n",
    "RANDOM_SEED    = 42                      # for reproducibility\n",
    "\n",
    "# 2. Load the full annotation JSON\n",
    "with open(os.path.join(DATASET_DIR, INPUT_FILE), 'r') as f:\n",
    "    full = json.load(f)\n",
    "\n",
    "images            = full['images']\n",
    "annotations       = full.get('annotations', [])\n",
    "scene_annotations = full.get('scene_annotations', [])\n",
    "info              = full.get('info', None)\n",
    "licenses          = full.get('licenses', [])\n",
    "categories        = full.get('categories', [])\n",
    "scene_categories  = full.get('scene_categories', [])\n",
    "\n",
    "# 3. Shuffle & split image list\n",
    "random.seed(RANDOM_SEED)\n",
    "random.shuffle(images)\n",
    "N = len(images)\n",
    "n_test = int(N * TEST_PCT + 0.5)\n",
    "n_val  = int(N * VAL_PCT  + 0.5)\n",
    "\n",
    "test_images  = images[:n_test]\n",
    "val_images   = images[n_test:n_test+n_val]\n",
    "train_images = images[n_test+n_val:]\n",
    "\n",
    "# 4. Build index sets for fast lookup\n",
    "test_ids  = {img['id'] for img in test_images}\n",
    "val_ids   = {img['id'] for img in val_images}\n",
    "train_ids = {img['id'] for img in train_images}\n",
    "\n",
    "# 5. Partition annotations\n",
    "def split_anns(anns, idset):\n",
    "    return [a for a in anns if a['image_id'] in idset]\n",
    "\n",
    "train_anns        = split_anns(annotations,       train_ids)\n",
    "val_anns          = split_anns(annotations,       val_ids)\n",
    "test_anns         = split_anns(annotations,       test_ids)\n",
    "train_scene_anns  = split_anns(scene_annotations, train_ids)\n",
    "val_scene_anns    = split_anns(scene_annotations, val_ids)\n",
    "test_scene_anns   = split_anns(scene_annotations, test_ids)\n",
    "\n",
    "# 6. Helper to dump one subset\n",
    "def dump_subset(name, imgs, anns, scene_anns):\n",
    "    out = {\n",
    "        'info':             info,\n",
    "        'licenses':         licenses,\n",
    "        'categories':       categories,\n",
    "        'scene_categories': scene_categories,\n",
    "        'images':           imgs,\n",
    "        'annotations':      anns,\n",
    "        'scene_annotations': scene_anns,\n",
    "    }\n",
    "    path = os.path.join(DATASET_DIR, f'annotations_{name}.json')\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(out, f)\n",
    "    print(f'Wrote {name}: {len(imgs)} images, {len(anns)} masks')\n",
    "\n",
    "# 7. Write three files\n",
    "dump_subset('train', train_images, train_anns, train_scene_anns)\n",
    "dump_subset('val',   val_images,   val_anns,   val_scene_anns)\n",
    "dump_subset('test',  test_images,  test_anns,  test_scene_anns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e7a238-9192-48e4-8c0f-185d9a46564f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2931090f-8ac5-4daf-a24a-4a467b642f92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os, random\n",
    "\n",
    "dataset_dir = 'data'\n",
    "ann_file    = os.path.join(dataset_dir, 'annotations_train.json')\n",
    "coco        = COCO(ann_file)\n",
    "img_ids     = coco.getImgIds()[:3]\n",
    "\n",
    "for img_id in img_ids:\n",
    "    # --- load image + anns ---\n",
    "    img = coco.loadImgs(img_id)[0]\n",
    "    I   = Image.open(os.path.join(dataset_dir, img['file_name']))\n",
    "    ann_ids = coco.getAnnIds(imgIds=[img_id])\n",
    "    anns    = coco.loadAnns(ann_ids)\n",
    "\n",
    "    # --- print how many + their categories ---\n",
    "    print(f\"\\nImage {img_id} ({img['file_name']}) has {len(anns)} masks:\")\n",
    "    for a in anns:\n",
    "        cat = coco.loadCats(a['category_id'])[0]\n",
    "        print(f\"  • {cat['name']} (super: {cat['supercategory']})\")\n",
    "\n",
    "    # --- set up figure ---\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,6))\n",
    "    fig.suptitle(f\"Image {img_id}\", fontsize=14)\n",
    "\n",
    "    ax1.imshow(I)\n",
    "    ax1.set_title(\"Original\")\n",
    "    ax1.axis('off')\n",
    "\n",
    "    ax2.imshow(I)\n",
    "    ax2.set_title(\"Colour‑coded masks\")\n",
    "    ax2.axis('off')\n",
    "\n",
    "    # --- overlay each mask in its own random colour ---\n",
    "    for ann in anns:\n",
    "        mask = coco.annToMask(ann)  # H×W binary\n",
    "        color = (random.random(), random.random(), random.random())\n",
    "        # make a H×W×3 RGB mask\n",
    "        rgb_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.float32)\n",
    "        for c in range(3):\n",
    "            rgb_mask[...,c] = mask * color[c]\n",
    "        ax2.imshow(rgb_mask, alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5cd3c0-d272-48df-9eb5-fdace219aa17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === CONFIGURE THIS ===\n",
    "dataset_dir = 'data'\n",
    "ann_file    = os.path.join(dataset_dir, 'annotations.json')\n",
    "\n",
    "# 1. Load your annotations\n",
    "with open(ann_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 2. Build a lookup for categories\n",
    "cats = {c['id']: c for c in data['categories']}\n",
    "\n",
    "# 3. Count up masks per category\n",
    "cat_counts = Counter()\n",
    "for ann in data['annotations']:\n",
    "    cat_counts[cats[ann['category_id']]['name']] += 1\n",
    "\n",
    "# sort descending\n",
    "cat_items = sorted(cat_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "names, counts = zip(*cat_items)\n",
    "\n",
    "# 3a. Print total masks and number of distinct categories\n",
    "total_masks      = sum(counts)\n",
    "num_categories   = len(names)\n",
    "print(f\"Total masks in dataset: {total_masks}\")\n",
    "print(f\"Number of categories   : {num_categories}\")\n",
    "\n",
    "# 4. Plot horizontal bar chart\n",
    "plt.figure(figsize=(8, 12))\n",
    "plt.barh(names, counts)\n",
    "plt.gca().invert_yaxis()   # largest at the top\n",
    "plt.xlabel(\"Number of Masks\")\n",
    "plt.title(\"Masks per Category\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89077d3c-a462-4dfc-83cf-c5829c2bd064",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === CONFIGURE THIS ===\n",
    "dataset_dir = 'data'\n",
    "ann_file    = os.path.join(dataset_dir, 'annotations.json')\n",
    "\n",
    "# 1. Load your annotations\n",
    "with open(ann_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 2. Build a lookup for categories\n",
    "cats = {c['id']: c for c in data['categories']}\n",
    "\n",
    "# 3. Count up masks per supercategory\n",
    "super_counts = Counter()\n",
    "for ann in data['annotations']:\n",
    "    supercat = cats[ann['category_id']]['supercategory']\n",
    "    super_counts[supercat] += 1\n",
    "\n",
    "# 3a. Sort descending\n",
    "items = sorted(super_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "names, counts = zip(*items)\n",
    "\n",
    "# 3b. Print totals\n",
    "total_masks        = sum(counts)\n",
    "num_supercats      = len(names)\n",
    "print(f\"Total masks in dataset      : {total_masks}\")\n",
    "print(f\"Number of supercategories   : {num_supercats}\\n\")\n",
    "\n",
    "# 4. Plot horizontal bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(names, counts)\n",
    "plt.gca().invert_yaxis()    # largest at the top\n",
    "plt.xlabel(\"Number of Masks\")\n",
    "plt.title(\"Masks per Supercategory\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30306587-979b-4d45-9afa-f3bb7cbb247a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04320e98-30bd-47c2-9978-7a5205fa7ac2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fc4d0b-9772-42fe-9ff4-ae3644181c2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de9aafd-72c6-4039-8ef7-eeb28d908876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bbe99f-1615-4da8-b4a5-39ec00f17fc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def clip_boxes_to_image(boxes, img_w, img_h):\n",
    "    clipped = []\n",
    "    for box in boxes:\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "        x_min = max(0, min(x_min, img_w - 1))\n",
    "        y_min = max(0, min(y_min, img_h - 1))\n",
    "        x_max = max(0, min(x_max, img_w - 1))\n",
    "        y_max = max(0, min(y_max, img_h - 1))\n",
    "        clipped.append([x_min, y_min, x_max, y_max])\n",
    "    return clipped\n",
    "\n",
    "class TacoDataset(Dataset):\n",
    "    def __init__(self, images_dir, annotation_path, transforms=None, max_resolution=(4000, 4000)):\n",
    "        self.images_dir = images_dir\n",
    "        self.coco = COCO(annotation_path)\n",
    "        self.transforms = transforms\n",
    "        self.max_resolution = max_resolution\n",
    "        self.image_ids = [img_id for img_id in self.coco.imgs\n",
    "                          if self.coco.imgs[img_id]['width'] <= max_resolution[0] and\n",
    "                             self.coco.imgs[img_id]['height'] <= max_resolution[1]]\n",
    "        print(f\"✅ Loaded {len(self.image_ids)} images under resolution {max_resolution}\")\n",
    "\n",
    "    import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def clip_boxes_to_image(boxes, img_w, img_h):\n",
    "    clipped = []\n",
    "    for box in boxes:\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "        x_min = max(0, min(x_min, img_w - 1))\n",
    "        y_min = max(0, min(y_min, img_h - 1))\n",
    "        x_max = max(0, min(x_max, img_w - 1))\n",
    "        y_max = max(0, min(y_max, img_h - 1))\n",
    "        clipped.append([x_min, y_min, x_max, y_max])\n",
    "    return clipped\n",
    "\n",
    "class TacoDataset(Dataset):\n",
    "    def __init__(self, images_dir, annotation_path, transforms=None, max_resolution=(4000, 4000)):\n",
    "        self.images_dir     = images_dir\n",
    "        self.coco           = COCO(annotation_path)\n",
    "        self.transforms     = transforms\n",
    "        self.max_resolution = max_resolution\n",
    "        # only keep images under max_resolution\n",
    "        self.image_ids = [\n",
    "            img_id for img_id, info in self.coco.imgs.items()\n",
    "            if info['width'] <= max_resolution[0] and info['height'] <= max_resolution[1]\n",
    "        ]\n",
    "        print(f\"✅ Loaded {len(self.image_ids)} images under resolution {max_resolution}\")\n",
    "\n",
    "    import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def clip_boxes_to_image(boxes, img_w, img_h):\n",
    "    clipped = []\n",
    "    for box in boxes:\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "        x_min = max(0, min(x_min, img_w - 1))\n",
    "        y_min = max(0, min(y_min, img_h - 1))\n",
    "        x_max = max(0, min(x_max, img_w - 1))\n",
    "        y_max = max(0, min(y_max, img_h - 1))\n",
    "        clipped.append([x_min, y_min, x_max, y_max])\n",
    "    return clipped\n",
    "\n",
    "class TacoDataset(Dataset):\n",
    "    def __init__(self, images_dir, annotation_path, transforms=None, max_resolution=(4000, 4000)):\n",
    "        self.images_dir     = images_dir\n",
    "        self.coco           = COCO(annotation_path)\n",
    "        self.transforms     = transforms\n",
    "        self.max_resolution = max_resolution\n",
    "        self.image_ids = [\n",
    "            img_id for img_id, info in self.coco.imgs.items()\n",
    "            if info['width']  <= max_resolution[0]\n",
    "            and info['height'] <= max_resolution[1]\n",
    "        ]\n",
    "        print(f\"✅ Loaded {len(self.image_ids)} images under resolution {max_resolution}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id   = self.image_ids[index]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.images_dir, img_info['file_name'])\n",
    "        image_np = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        img_h, img_w = image_np.shape[:2]\n",
    "\n",
    "        # load annotations\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns    = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        # build raw boxes, labels, masks lists\n",
    "        boxes, labels, masks = [], [], []\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            x_min, y_min = x, y\n",
    "            x_max, y_max = x + w, y + h\n",
    "            if x_max <= x_min or y_max <= y_min:\n",
    "                continue\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            labels.append(ann['category_id'])\n",
    "            masks.append(self.coco.annToMask(ann))\n",
    "\n",
    "        # clamp boxes\n",
    "        boxes = clip_boxes_to_image(boxes, img_w, img_h)\n",
    "\n",
    "        # resize masks\n",
    "        resized_masks = []\n",
    "        for m in masks:\n",
    "            if m.shape != (img_h, img_w):\n",
    "                m_img     = Image.fromarray(m.astype(np.uint8))\n",
    "                m_resized = F.resize(m_img, (img_h, img_w), interpolation=Image.NEAREST)\n",
    "                m         = np.array(m_resized)\n",
    "            resized_masks.append(m)\n",
    "\n",
    "        # apply transforms if any\n",
    "        if self.transforms:\n",
    "            try:\n",
    "                transformed = self.transforms(\n",
    "                    image=image_np,\n",
    "                    masks=resized_masks,\n",
    "                    bboxes=boxes,\n",
    "                    category_ids=labels\n",
    "                )\n",
    "                image = transformed['image'].float() / 255.0\n",
    "\n",
    "                boxes  = torch.as_tensor(transformed['bboxes'], dtype=torch.float32)\n",
    "                labels = torch.as_tensor(transformed['category_ids'], dtype=torch.int64)\n",
    "                masks  = torch.stack([torch.tensor(m, dtype=torch.uint8)\n",
    "                                      for m in transformed['masks']])\n",
    "                \n",
    "                # filter out tiny/invalid\n",
    "                keep = []\n",
    "                for i, box in enumerate(boxes):\n",
    "                    if (box[2]-box[0] > 1) and (box[3]-box[1] > 1):\n",
    "                        keep.append(i)\n",
    "                if len(keep) < boxes.size(0):\n",
    "                    boxes  = boxes[keep]\n",
    "                    labels = labels[keep]\n",
    "                    masks  = masks[keep]\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Transform failed on image_id {img_id}: {e}\")\n",
    "                image  = F.to_tensor(Image.fromarray(image_np))\n",
    "                boxes  = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "                labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "                masks  = torch.as_tensor(np.stack(resized_masks), dtype=torch.uint8)\n",
    "        else:\n",
    "            image  = F.to_tensor(Image.fromarray(image_np))\n",
    "            boxes  = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            masks  = torch.as_tensor(np.stack(resized_masks), dtype=torch.uint8)\n",
    "\n",
    "        # —— Remove any degenerate boxes (zero width or height) ——\n",
    "        if boxes.numel() > 0:\n",
    "            widths  = boxes[:, 2] - boxes[:, 0]\n",
    "            heights = boxes[:, 3] - boxes[:, 1]\n",
    "            keep    = (widths > 0) & (heights > 0)\n",
    "            if keep.sum() < boxes.size(0):\n",
    "                boxes  = boxes[keep]\n",
    "                labels = labels[keep]\n",
    "                masks  = masks[keep]\n",
    "\n",
    "        # —— Handle empty targets —— \n",
    "        if boxes.numel() == 0:\n",
    "            boxes   = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels  = torch.zeros((0,),    dtype=torch.int64)\n",
    "            masks   = torch.zeros((0, img_h, img_w), dtype=torch.uint8)\n",
    "            areas   = torch.zeros((0,),    dtype=torch.float32)\n",
    "            iscrowd = torch.zeros((0,),    dtype=torch.int64)\n",
    "        else:\n",
    "            areas   = torch.as_tensor([ann['area']      for ann in anns], dtype=torch.float32)\n",
    "            iscrowd = torch.as_tensor([ann.get('iscrowd', 0) for ann in anns], dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes':    boxes,\n",
    "            'labels':   labels,\n",
    "            'masks':    masks,\n",
    "            'image_id': torch.tensor([img_id]),\n",
    "            'area':     areas,\n",
    "            'iscrowd':  iscrowd\n",
    "        }\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e05a6bb-4384-48d2-b576-3bc705ba06bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ✅ New augmentations using Detectron2-style (simple and robust)\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(512, 512),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        ToTensorV2()\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category_ids']))\n",
    "\n",
    "\n",
    "def get_val_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(256, 256),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "# Dataset & DataLoader setup\n",
    "train_dataset = TacoDataset(\n",
    "    images_dir='data',\n",
    "    annotation_path='data/annotations_train.json',\n",
    "    transforms=get_train_transform()\n",
    ")\n",
    "\n",
    "val_dataset = TacoDataset(\n",
    "    images_dir='data',\n",
    "    annotation_path='data/annotations_val.json',\n",
    "    transforms=get_val_transform()\n",
    ")\n",
    "\n",
    "test_dataset = TacoDataset(\n",
    "    images_dir='data',\n",
    "    annotation_path='data/annotations_test.json',\n",
    "    transforms=get_val_transform()\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"\\n📦 Train: {len(train_dataset)} images\")\n",
    "print(f\"📦 Val:   {len(val_dataset)} images\")\n",
    "print(f\"📦 Test:  {len(test_dataset)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985454d4-4a1d-4fe1-9edc-c25b9d2d24e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "def show_image_with_boxes_masks(image, boxes, masks, title=\"\"):\n",
    "    img = image.copy()\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "\n",
    "    for mask in masks:\n",
    "        img[mask > 0] = img[mask > 0] * 0.5 + np.array([255, 0, 0]) * 0.5  # overlay red mask\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(img.astype(np.uint8))\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Pick a random image\n",
    "index = random.randint(0, len(train_dataset) - 1)\n",
    "\n",
    "# Load without transform\n",
    "original = TacoDataset(\n",
    "    images_dir='data',\n",
    "    annotation_path='data/annotations_train.json',\n",
    "    transforms=None\n",
    ")[index]\n",
    "orig_img = original[0].permute(1, 2, 0).numpy() * 255\n",
    "orig_boxes = original[1]['boxes'].numpy()\n",
    "orig_masks = original[1]['masks'].numpy()\n",
    "\n",
    "# Load with transform\n",
    "augmented = train_dataset[index]\n",
    "aug_img = augmented[0].permute(1, 2, 0).numpy() * 255\n",
    "aug_boxes = augmented[1]['boxes'].numpy()\n",
    "aug_masks = augmented[1]['masks'].numpy()\n",
    "\n",
    "# Show before\n",
    "show_image_with_boxes_masks(orig_img, orig_boxes, orig_masks, title=\"Before Augmentation\")\n",
    "\n",
    "# Show after\n",
    "show_image_with_boxes_masks(aug_img, aug_boxes, aug_masks, title=\"After Augmentation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809380e1-b085-41df-bfa8-83d89048032c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for imgs, targets in train_loader:\n",
    "    print(\"Batch size:\", len(imgs))\n",
    "    print(\"1st image shape:\", imgs[0])\n",
    "    print(\"1st target keys:\", targets[0].keys())\n",
    "    print(\"1st target values:\", targets[0].values())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85532e15-6c2e-4b81-9d12-c703874cbe8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad4902d-2dd6-42d8-a0e3-a3a8280a6869",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# --- Create the Mask R-CNN model ---\n",
    "def get_model(num_classes):\n",
    "    model = maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # Replace box head\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Replace mask head\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, 256, num_classes)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a06da69-2886-46fe-9044-2412c9b88dab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# --- Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = len(train_dataset.coco.getCatIds()) + 1  # +1 for background\n",
    "model = get_model(num_classes).to(device)\n",
    "\n",
    "# use the same optimizer\n",
    "optimizer = optim.SGD(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    lr=0.005,             # bump up to 0.005 (the torchvision default)\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "# one-cycle LR scheduler for num_epochs * steps_per_epoch total steps\n",
    "steps_per_epoch = len(train_loader)\n",
    "num_epochs=10\n",
    "lr_scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=0.005,\n",
    "    total_steps = num_epochs * steps_per_epoch,\n",
    "    pct_start = 0.1,      # 10% of the cycle is warm-up\n",
    "    anneal_strategy = 'cos', \n",
    "    div_factor = 25.0,    # start LR = max_lr/div_factor\n",
    "    final_div_factor = 10000.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95653340-81cf-4db8-96f1-071989767f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage import measure\n",
    "\n",
    "# def binary_mask_to_polygon(mask, tolerance=2):\n",
    "#     if mask.shape[0] < 2 or mask.shape[1] < 2:\n",
    "#         print(f\"⚠️ Skipping too-small mask with shape: {mask.shape}\")\n",
    "#         return []  # Skip invalid masks\n",
    "\n",
    "#     contours = measure.find_contours(mask, 0.5)\n",
    "#     segmentations = []\n",
    "\n",
    "#     for contour in contours:\n",
    "#         contour = np.flip(contour, axis=1)  # (y,x) → (x,y)\n",
    "#         segmentation = contour.ravel().tolist()\n",
    "#         if len(segmentation) >= 6:  # must have at least 3 points\n",
    "#             segmentations.append(segmentation)\n",
    "\n",
    "#     return segmentations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defd3e40-f779-490f-b8db-f960d4aa59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pycocotools.cocoeval import COCOeval\n",
    "# import numpy as np\n",
    "\n",
    "# def evaluate_model(model, data_loader, coco_gt, device, coco_category_ids):\n",
    "#     model.eval()\n",
    "#     coco_results = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, targets in data_loader:\n",
    "#             images = [img.to(device) for img in images]\n",
    "#             outputs = model(images)\n",
    "\n",
    "#             for i, output in enumerate(outputs):\n",
    "#                 img_id = int(targets[i][\"image_id\"].item())\n",
    "#                 boxes = output[\"boxes\"].detach().cpu().numpy()\n",
    "#                 scores = output[\"scores\"].detach().cpu().numpy()\n",
    "#                 labels = output[\"labels\"].detach().cpu().numpy()\n",
    "#                 masks = output[\"masks\"].detach().cpu().numpy()[:, 0, :, :] \n",
    "\n",
    "#                 for j in range(len(boxes)):\n",
    "#                     # if scores[j] < 0.05:\n",
    "#                     #     continue\n",
    "\n",
    "#                     x1, y1, x2, y2 = boxes[j]\n",
    "#                     if x2 <= x1 or y2 <= y1:\n",
    "#                         continue\n",
    "\n",
    "#                     polygons = binary_mask_to_polygon(masks[j])\n",
    "#                     if not polygons:\n",
    "#                         continue\n",
    "\n",
    "#                     coco_results.append({\n",
    "#                         \"image_id\": img_id,\n",
    "#                         \"category_id\": int(coco_category_ids.get(labels[j], labels[j])),  # map label\n",
    "#                         \"bbox\": [x1, y1, x2 - x1, y2 - y1],\n",
    "#                         \"score\": 1.0,\n",
    "#                         \"segmentation\": polygons\n",
    "#                     })\n",
    "\n",
    "#     if not coco_results:\n",
    "#         print(\"⚠️ No valid predictions to evaluate.\")\n",
    "#         return\n",
    "\n",
    "#     coco_dt = coco_gt.loadRes(coco_results)\n",
    "#     coco_eval = COCOeval(coco_gt, coco_dt, iouType='segm')\n",
    "#     coco_eval.evaluate()\n",
    "#     coco_eval.accumulate()\n",
    "#     coco_eval.summarize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e3eacc-7b65-4267-b8b0-6e2144d24d78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "def calculate_iou(mask1: torch.Tensor, mask2: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Compute IoU between two binary masks.\n",
    "    \"\"\"\n",
    "    intersection = torch.logical_and(mask1, mask2).sum().item()\n",
    "    union        = torch.logical_or(mask1, mask2).sum().item()\n",
    "    return intersection / union if union > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b7ef9-2848-49a7-89d7-ea1473be096e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_segmentation_metrics(\n",
    "    model,\n",
    "    data_loader,\n",
    "    device\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate Mask R-CNN on a dataset, computing precision, recall, mean IoU, and class accuracy.\n",
    "    - Filters predictions with score < score_thresh.\n",
    "    - Binarizes masks at mask_thresh.\n",
    "    - Handles images with zero GT or zero predictions.\n",
    "    - Class accuracy = (# correct GT–pred matches) / (# GT + # unmatched preds).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    all_ious   = []\n",
    "    cls_correct, cls_total = 0, 0\n",
    "\n",
    "    score_thresh = 0.5\n",
    "    mask_thresh  = 0.5\n",
    "    iou_threshold= 0.5\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images  = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            outputs = model(images)\n",
    "\n",
    "            for i in range(len(images)):\n",
    "                # --- Ground truth ---\n",
    "                gt_masks  = targets[i]['masks'].bool()   # [G, H, W]\n",
    "                gt_labels = targets[i]['labels']         # [G]\n",
    "\n",
    "                # --- Predictions (score + mask threshold) ---\n",
    "                scores     = outputs[i]['scores']        # [P]\n",
    "                keep_pred  = scores > score_thresh\n",
    "                pred_masks = outputs[i]['masks'][keep_pred, 0] > mask_thresh  # [P, H, W]\n",
    "                pred_labels= outputs[i]['labels'][keep_pred]                # [P]\n",
    "\n",
    "                # --- Special cases ---\n",
    "                if gt_masks.shape[0] == 0:\n",
    "                    # no GT => all preds are false positives\n",
    "                    fp += pred_masks.shape[0]\n",
    "                    continue\n",
    "                if pred_masks.shape[0] == 0:\n",
    "                    # no preds => all GT are false negatives\n",
    "                    fn += gt_masks.shape[0]\n",
    "                    continue\n",
    "\n",
    "                # --- Greedy 1:1 matching by IoU & class ---\n",
    "                matched_pred = set()\n",
    "                for gi, (gt_m, gt_cls) in enumerate(zip(gt_masks, gt_labels)):\n",
    "                    best_iou = 0.0\n",
    "                    best_pi  = -1\n",
    "                    for pi, (pm, p_cls) in enumerate(zip(pred_masks, pred_labels)):\n",
    "                        if pi in matched_pred or p_cls != gt_cls:\n",
    "                            continue\n",
    "                        iou = calculate_iou(gt_m, pm)\n",
    "                        if iou > best_iou:\n",
    "                            best_iou = iou\n",
    "                            best_pi  = pi\n",
    "\n",
    "                    if best_iou >= iou_threshold:\n",
    "                        # True positive\n",
    "                        tp += 1\n",
    "                        all_ious.append(best_iou)\n",
    "                        matched_pred.add(best_pi)\n",
    "                        # Classification was correct\n",
    "                        cls_correct += 1\n",
    "                        cls_total   += 1\n",
    "                    else:\n",
    "                        # Missed this GT\n",
    "                        fn += 1\n",
    "                        cls_total += 1\n",
    "\n",
    "                # Unmatched preds are false positives + classification errors\n",
    "                num_unmatched = pred_masks.shape[0] - len(matched_pred)\n",
    "                fp += num_unmatched\n",
    "                cls_total += num_unmatched\n",
    "\n",
    "    # ——— Metrics ———\n",
    "    precision    = tp / (tp + fp + 1e-6)\n",
    "    recall       = tp / (tp + fn + 1e-6)\n",
    "    mean_iou     = np.mean(all_ious) if all_ious else 0.0\n",
    "    cls_accuracy = cls_correct / (cls_total + 1e-6)\n",
    "\n",
    "    return {\n",
    "        \"mean_iou\":      mean_iou,\n",
    "        \"precision\":     precision,\n",
    "        \"recall\":        recall,\n",
    "        \"cls_accuracy\":  cls_accuracy\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d0aa06-2862-48cc-bfbd-5c3b3bc780ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, val_loader, test_loader, optimizer, lr_scheduler, device, num_epochs=10, print_interval=40):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        print(f\"\\n🚀 Starting Epoch {epoch+1}/{num_epochs} at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "\n",
    "        for step, (images, targets) in enumerate(train_loader):\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            try:\n",
    "                loss_dict = model(images, targets)\n",
    "                loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(f\"⚠️ Invalid loss at step {step+1}, skipping\")\n",
    "                    continue\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            except RuntimeError as e:\n",
    "                print(f\"❌ CUDA error at step {step+1}: {e}\")\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "\n",
    "            if (step + 1) % print_interval == 0 or (step + 1) == len(train_loader):\n",
    "                print(f\"  🔁 Step {step+1}/{len(train_loader)} — Batch Loss: {loss.item():.4f}\")\n",
    "\n",
    "            del images, targets\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"\\n📘 Epoch {epoch+1} — Average Epoch Loss: {avg_loss:.4f}\")\n",
    "        if epoch % 2 == 0:\n",
    "            for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "                print(f\"\\n🔍 Evaluating on {name} set...\")\n",
    "                try:\n",
    "                    metrics = evaluate_segmentation_metrics(model, loader, device)\n",
    "                    print(f\"📏 {name.upper()} → IoU: {metrics['mean_iou']:.4f} | \"\n",
    "                          f\"Precision: {metrics['precision']:.4f} | Recall: {metrics['recall']:.4f} | \"\n",
    "                          f\"Class Acc: {metrics['cls_accuracy']:.4f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Evaluation failed on {name}: {e}\")\n",
    "\n",
    "    print(\"\\n🔍 Final Evaluation on TEST set...\")\n",
    "    metrics_test = evaluate_segmentation_metrics(model, test_loader, device)\n",
    "    print(f\"📏 TEST → IoU: {metrics_test['mean_iou']:.4f} | \"\n",
    "          f\"Precision: {metrics_test['precision']:.4f} | Recall: {metrics_test['recall']:.4f} | \"\n",
    "          f\"Class Acc: {metrics_test['cls_accuracy']:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"maskrcnn_taco_baseline.pth\")\n",
    "    print(\"✅ Model saved to maskrcnn_taco_baseline.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cdf592-b1ef-4d6e-af09-8a1943da1ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_and_evaluate(model, train_loader, val_loader, test_loader, optimizer, lr_scheduler, device, num_epochs=10, print_interval=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e83363c-94ad-42fe-a24a-bd78ba82d130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cea25a9-88bc-4837-abf3-676ad279ad2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from IPython.display import Image as IPImage, display\n",
    "\n",
    "def print_predictions_on_test_set(\n",
    "    model,\n",
    "    test_loader,\n",
    "    dataset,\n",
    "    device,\n",
    "    score_thresh: float = 0.5,\n",
    "    mask_thresh:  float = 0.5,\n",
    "    save_dir:     str   = \"predicted_masks\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the model on the test_loader, then for each image:\n",
    "     - Filters predictions by score_thresh\n",
    "     - Binarizes masks at mask_thresh\n",
    "     - Overlays masks in red and draws boxes in yellow\n",
    "     - Saves and displays each result inline\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Map COCO category IDs to names\n",
    "    category_id_to_name = {\n",
    "        cat['id']: cat['name']\n",
    "        for cat in dataset.coco.loadCats(dataset.coco.getCatIds())\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            # Move to device\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "\n",
    "            for img_tensor, output, target in zip(images, outputs, targets):\n",
    "                img_id  = int(target['image_id'].item())\n",
    "                boxes   = output['boxes'].cpu().numpy()\n",
    "                scores  = output['scores'].cpu().numpy()\n",
    "                labels  = output['labels'].cpu().numpy()\n",
    "                masks   = (output['masks'][:, 0].cpu().numpy() > mask_thresh)\n",
    "\n",
    "                # Convert tensor to H×W×3 uint8 image\n",
    "                img_np = img_tensor.cpu().permute(1, 2, 0).numpy()\n",
    "                img_np = (img_np * 255).astype(np.uint8)\n",
    "\n",
    "                # Create a figure & axis\n",
    "                fig, ax = plt.subplots(figsize=(6, 6))\n",
    "                ax.imshow(img_np)\n",
    "\n",
    "                any_pred = False\n",
    "                for j, (box, score, label_id, mask) in enumerate(\n",
    "                    zip(boxes, scores, labels, masks)\n",
    "                ):\n",
    "                    if score < score_thresh:\n",
    "                        continue\n",
    "                    any_pred = True\n",
    "\n",
    "                    # Draw bounding box in yellow\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    rect = patches.Rectangle(\n",
    "                        (x1, y1),\n",
    "                        x2 - x1,\n",
    "                        y2 - y1,\n",
    "                        linewidth=2,\n",
    "                        edgecolor=\"yellow\",\n",
    "                        facecolor=\"none\"\n",
    "                    )\n",
    "                    ax.add_patch(rect)\n",
    "\n",
    "                    # Overlay mask in transparent red\n",
    "                    red_overlay = img_np.copy()\n",
    "                    red_overlay[mask] = (\n",
    "                        red_overlay[mask] * 0.5 + np.array([255, 0, 0]) * 0.5\n",
    "                    )\n",
    "                    ax.imshow(red_overlay, alpha=0.5)\n",
    "\n",
    "                    # Print caption\n",
    "                    label_name = category_id_to_name.get(label_id, f\"Class {label_id}\")\n",
    "                    ax.text(\n",
    "                        x1,\n",
    "                        y1 - 5,\n",
    "                        f\"{label_name}: {score:.2f}\",\n",
    "                        color=\"yellow\",\n",
    "                        fontsize=12,\n",
    "                        backgroundcolor=\"black\",\n",
    "                        alpha=0.7\n",
    "                    )\n",
    "\n",
    "                if not any_pred:\n",
    "                    ax.text(\n",
    "                        10,\n",
    "                        20,\n",
    "                        \"⚠️ No predictions above threshold\",\n",
    "                        color=\"red\",\n",
    "                        fontsize=14,\n",
    "                        backgroundcolor=\"white\"\n",
    "                    )\n",
    "\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "                # Save and display\n",
    "                out_path = os.path.join(save_dir, f\"img_{img_id}.png\")\n",
    "                fig.savefig(out_path, bbox_inches=\"tight\", pad_inches=0)\n",
    "                plt.close(fig)\n",
    "\n",
    "                display(IPImage(out_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa6dfd-6f3e-4267-9555-7343ed8036a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_predictions_on_test_set(model, test_loader, test_dataset, device, score_threshold=0.5, show_masks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bcf941-ecbc-49f8-8f46-2be3c8d08977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be8e05e-a14f-4a15-a515-53b13bc7858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools import mask as maskUtils\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_ground_truth_as_prediction(coco_gt):\n",
    "    coco_results = []\n",
    "\n",
    "    for img_id in coco_gt.getImgIds():\n",
    "        anns = coco_gt.loadAnns(coco_gt.getAnnIds(imgIds=img_id))\n",
    "        for ann in anns:\n",
    "            if 'segmentation' not in ann or 'bbox' not in ann or 'category_id' not in ann:\n",
    "                continue\n",
    "\n",
    "            # COCO format expects score when evaluating detections\n",
    "            coco_results.append({\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": ann[\"category_id\"],\n",
    "                \"bbox\": ann[\"bbox\"],\n",
    "                \"score\": 1.0,  # max confidence\n",
    "                \"segmentation\": ann[\"segmentation\"]\n",
    "            })\n",
    "\n",
    "    # Convert to COCO predictions format\n",
    "    coco_dt = coco_gt.loadRes(coco_results)\n",
    "\n",
    "    # Run evaluation\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType='segm')\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70eb00a-a80e-4185-94da-d647d4b196c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_ground_truth_as_prediction(val_dataset.coco)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f55c48-ca35-4d86-a135-424c4b6d4efe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu124.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu124:m129"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
